---
title: webmagic源码解析【1】
tags: [webmagic]
categories: "源码解析"
author: yip6
---

## 前言 ##
 一年前用过webmagic这个开源爬虫， 使用过程中感受到了它给个人开发带来的便利，但因为时间限制没有能够深入源码学习。近期利用业余时间学习源码的同时记录下学习的过程。本文将尝试由自顶向下的顺序介绍webmagic的工作流程，并深入探讨一些实现的原理。

## 整体分析 ##
#### 主要组件 ####
Spider做为对于爬虫的抽象，成员变量包括`Downloader`,`PageProcesser`,`Scheduler`,`Pipeline`
分别负责下载页面、分析页面、url的管理、结果处理。

#### 测试案例 ####
```java
public class ZhihuPageProcessor implements PageProcessor{

    private Site site = Site.me().setRetryTimes(3).setSleepTime(1000);

    //针对页面定制处理规则
    @Override
    public void process(Page page) {
        page.putField("title",page.getHtml().xpath("//h1[@class='PostIndex-title']/text()"));
        if(page.getResultItems().get("title") == null){
            page.setSkip(true);
        }
    }

    @Override
    public Site getSite() {
        return site;
    }

    public static void main(String[] args){
        Spider.create(new ZhihuPageProcessor())
                .addUrl("https://zhuanlan.zhihu.com/p/31820191")
                .run();
    }
}
```
main函数中调用`Spider`的`create(PageProcesser pageProcesser)`方法会调用Spider的构造函数，构造spider实例
```java
public Spider(PageProcessor pageProcessor) {
        //初始化pageProcessor
        this.pageProcessor = pageProcessor;
        this.site = pageProcessor.getSite();
        this.startRequests = pageProcessor.getSite().getStartRequests();
    }
```
注意代码的链式调用。调用addUrl方法，将*种子url*放进scheduler中进行管理
```java
public Spider addUrl(String... urls) {
        for (String url : urls) {
            addRequest(new Request(url));  //将url放进scheduler中进行管理
        }
        signalNewUrl();
        return this;
    }
```
```java
private void addRequest(Request request) {
        if (site.getDomain() == null && request != null && request.getUrl() != null) {
            site.setDomain(UrlUtils.getDomain(request.getUrl()));
        }
        //scheduler的去重复是通过set这个结构完成的
        scheduler.push(request, this);
    }
```
`Spider`实现了`Runnable`接口，说明爬虫可以是多线程的，并发管理我们会在后面提及，这里先从`run()`中的业务流程进行解析。
`checkRunningStat()`检查线程的运行状态，非`STAT_RUNNING`的线程则初始化前面提到的`downloader`、`pipeline`。初始化完成后，从`scheduler`中取出一个未爬取的url,进行处理。
```java
protected void processRequest(Request request) {
        //下载页面
        Page page = downloader.download(request, this);
        if (page == null) {
            sleep(site.getRetrySleepTime());
            onError(request);
            return;
        }
     
        if (page.isNeedCycleRetry()) {
            extractAndAddRequests(page, true);
            sleep(site.getRetrySleepTime());
            return;
        }
        //处理页面
        pageProcessor.process(page);
        //抽取页面中包含的url
        extractAndAddRequests(page, spawnUrl);
        if (!page.getResultItems().isSkip()) {
            for (Pipeline pipeline : pipelines) {
                //分析页面后处理得到的数据
                pipeline.process(page.getResultItems(), this);
            }
        }
        
        request.putExtra(Request.STATUS_CODE, page.getStatusCode());
        sleep(site.getSleepTime());
    }

```
至此，一次完整的爬取流程结束。

